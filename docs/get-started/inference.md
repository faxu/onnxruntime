---
title: Inference
parent: Get Started
nav_order: 2
---

# Use ONNX Runtime for Inferencing

[ONNX Runtime inference](get-started/inference.md) APIs are stable and production-ready since the October 2019 [1.0 release](https://github.com/microsoft/onnxruntime/releases/tag/v1.0.0). ONNX Runtime powers machine learning models in key Microsoft products and services across Office, Azure, Bing, and more, as well as dozens of community projects. 

Some examples of problems that ONNX Runtime can help with:

* Improve inference performance for a wide variety of ML models (see [acceleration tutorials](./tutorials/acceleration))
* Run on different hardware and operating systems [see [portability tutorials](./tutorials/portability)]
* Train in Python but deploy into a C#/C++/Java app
* Train and perform inference with models created in different frameworks


