---
title: Acceleration
parent: Tutorials
has_children: false
nav_order: 2
---
# Tutorials: Acceleration

## Inferencing

* [Inference: accelerate huggingface transformer model (GPT2)](https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/notebooks/Inference_GPT2_with_OnnxRuntime_on_CPU.ipynb)
* [Inference: accelerate BERT model (TF) on CPU](https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/notebooks/PyTorch_Bert-Squad_OnnxRuntime_CPU.ipynb)
* [Inference: accelerate BERT model (TF) on GPU](https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/notebooks/PyTorch_Bert-Squad_OnnxRuntime_GPU.ipynb)
* [Inference: accelerate reduced size BERT model (PyTorch) through quantization](https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/quantization/notebooks/Bert-GLUE_OnnxRuntime_quantization.ipynb)
* [Inference: accelerate model inferencing using just-in-time compilation](https://github.com/microsoft/onnxruntime/blob/master/docs/python/notebooks/onnxruntime-nuphar-tutorial.ipynb)

## Training

* [Training: accelerate pre-training of large BERT model](https://github.com/microsoft/onnxruntime-training-examples/tree/master/nvidia-bert)
* [Training: accelerate fine tuning of Huggingface GPT2 model](https://github.com/microsoft/onnxruntime-training-examples/tree/master/huggingface-gpt2)