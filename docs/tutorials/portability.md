---
title: Portability
parent: Tutorials
has_children: false
nav_order: 3
---

# Tutorials: Portability
ONNX Runtime's compatibility with different model frameworks, development environments, and hardware accelerators provides a unified  solution for diverse use cases. Below are a few common examples of how ONNX Runtime can be used and deployed.

## General
* *[COMING SOON]* Deploy a Python-trained model in a C# environment
* *[COMING SOON]* Deploy a scikit-learn model securely without pkl files
* *[COMING SOON]* Run traditional ML and DNN models with a single runtime integration

## Windows-inbox
* [Model inferencing natively on Windows](https://docs.microsoft.com/en-us/windows/ai/windows-ml/)
  * [Tutorials for Windows Desktop or UWP app](https://docs.microsoft.com/en-us/windows/ai/windows-ml/get-started-desktop)

## IoT/Edge
* [Fast model inferencing on Jetson Nano embedded device](https://github.com/Azure-Samples/onnxruntime-iot-edge/blob/master/README-ONNXRUNTIME-arm64.md)
* [Deploy small quantized model on Intel VPU edge device with OpenVINO acceleration](https://github.com/Azure-Samples/onnxruntime-iot-edge/blob/master/README-ONNXRUNTIME-OpenVINO.md)

## Mobile
* *[COMING SOON]* Small and fast model inferencing on Android
* *[COMING SOON]* Small and fast model inferencing on iOS

## Web
 *[COMING SOON]* 