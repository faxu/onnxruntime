---
title: Accelerate PyTorch
grand_parent: Tutorials
parent: Inferencing
nav_order: 2
---
# Accelerate PyTorch model inferencing
{: .no_toc }

ONNX Runtime can be used to accelerate PyTorch models inferencing.

## Contents
{: .no_toc }

* TOC placeholder
{:toc}

## Convert model to ONNX

* [Basic PyTorch export to ONNX through torch.onnx](https://pytorch.org/docs/stable/onnx.html)
* [Export PyTorch model with custom ops](../tutorials/export-pytorch-model.md)

## Accelerate PyTorch model inferencing

* [Accelerate reduced size BERT model through quantization](https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/quantization/notebooks/bert/Bert-GLUE_OnnxRuntime_quantization.ipynb)
